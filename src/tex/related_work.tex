\section{Обзор существующих методов}

Существует много алгоритмов для решения задачи семантической сегментации или задачи поиска контура. Сюда входят, в том числе, методы активного контура~\cite{snakes}, сегментация с помощью кластеризации~\cite{clustering_segm} и нейросетевые методы~\cite{fcn},~\cite{unet},~\cite{gridnet}.

\subsection{Простая сверточная модель}

Простая сверточная модель~\cite{fcn_1_layer_upsample} была разработана на основе сверточных нейронных сетей для классификации. Отличие состоит в том, что в сетях для класификации последний слой softmax~\cite{classification_loss} преобразует высокоуровневые признаки, полученные последовательным применением сверток в вероятности принадлежности к классам, а~для~сегментации сначала используется upsampling слой, который преобразует высокоуровневые признаки в~изображение размером с~исходное, которое описывает, где именно находится объект. 

Каждый сверточный блок состоит из~нескольких слоев, расположенных друг за~другом:

\begin{center}
	\begin{tabular}{ c }
	  \hline
	  сверточный слой с~ядром~3x3   		\\ \hline
	  mvn слой 								\\ \hline
	  сверточный слой с~ядром~3x3			\\ \hline
	  mvn слой 								\\ \hline
	  \dots									\\ \hline
	  maxpool слой с~ядром~2x2 и~шагом~2 	\\ 
	  \hline
	\end{tabular}
\end{center}

Такая структура часто используется в нейронных сетях, потому что позволяет ускорить сходимость и~уменьшить количество вычислений при~сохранении точности. Сверточные слои обеспечивают вычисление высокоуровневых признаков с помощью применения фильтров к входному изображению. Maxpool слои позволяют уменьшить сложность вычислений, и оставить самую важную информацию, полученную из сверточных слоев. Слой dropout помогает избежать переобучения~(\cite{dropout}). 

\subsection{Модель c обратной сверткой}
 
Модель с обратной сверткой~\cite{fcn} более сложная. Она состоит из~нескольких сверточных блоков и~нескольких обратных сверточных блоков. Каждый блок, в~свою очередь, состоит из~нескольких сверточных слоев с~функцией активации relu и~maxpool~слоя. 

Всего в~нейронной сети 3~сверточных блока и~1~блок без~maxpool слоя. Они идут подряд, и~каждый следующий на~выходе дает в~2~раза больше фильтров. 

Структура обратного блока, напротив, нелинейная:

\begin{itemize}
  \item обратный сверточный слой с~ядром~3x3 и~шагом~2,
  \item сверточный слой с~ядром~1х1 и~шагом~1, который вычисляется от~предыдущего mvn~слоя,
  \item слой, который вычисляет среднее арифметическое из~первых двух слоев блока.
\end{itemize}

Всего в нейронной сети 3~обратных блока. Количество фильтров в каждой свертке обратного блока равно количеству классов. Если же класса всего~2, то мы задаем на выходе 1~фильтр и~в~конце используем функцию активации sigmoid, вычисляя ошибку при этом с~помощью индекса Дайса. В данной работе во~всех случаях используется только 2~класса.

Обозначим сверточный блок с~$k$~свертками и~$n$~фильтрами как~$conv(k,n)$. Обозначим обратный сверточный слой, который получает на одном из входов $n$~фильтров как~$deconv(n)$. Обозначим слой dropout регуляризации с параметром $p$ как $dropout(p)$. Таким образом, вся архитектура сети выглядит следующим образом:

\begin{itemize}
  \item Вход
  \item $conv(3,64)$
  \item $conv(4,128)$
  \item $conv(4,256)$
  \item $droupout(0.5)$
  \item $conv(4,512)$ без maxpool слоя
  \item $droupout(0.5)$
  \item $deconv(512)$
  \item $deconv(256)$
  \item $deconv(128)$
  \item $sigmoid$ активация
\end{itemize}

Чтобы сеть сходилась устойчиво, после каждой свертки используется mvn слой~(\cite{batch_norm}). Обратные сверточные слои позволяют получить карту вероятностей принадлежности входных пикселей объекту в~зависимости от~контекста из~высокоуровневых признаков, вычисленных сверточными слоями. Каждый блок обратной свертки связан не только с предыдущим блоком обратной свертки, но и с соответствующим по размеру сверточным блоком, для того, чтобы обеспечить сходимость сети. Несколько обратных слоев позволяют сгладить резкие переходы на~выходной карте вероятностей и~получить более точные предсказания. В~обратных блоках учитываются выходы соответствующих mvn слоев прямых блоков, чтобы увеличить скорость обучения. Этот прием очень похож на~residual блоки в~\cite{resnet}.

\subsection{U-Net}

Архитектура U-Net \cite{unet} достаточно сильно отличается от простой модели. Здесь также используются прямые и обратные блоки. Каждый прямой блок U-Net состоит из 2-х сверточных слоев с ядром 3х3 и функцией активации relu. Каждый обратный блок для увеличения ширины и высоты тензора использует upsampling слои, которые просто

\subsection{GridNet}
