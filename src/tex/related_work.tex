\section{Обзор существующих методов}

Существует много алгоритмов для решения задачи семантической сегментации или задачи поиска контура. Сюда входят, в том числе, методы активного контура~\cite{snakes}, сегментация с помощью кластеризации~\cite{clustering_segm} и нейросетевые методы~\cite{fcn},~\cite{unet},~\cite{gridnet}.

\subsection{Простая сверточная сеть}
 
Простая модель~\cite{fcn} состоит из~нескольких сверточных блоков и~нескольких обратных сверточных блоков. Каждый блок, в~свою очередь, состоит из~нескольких сверточных слоев и~слоя max~pooling. 

Каждый сверточный блок состоит из~нескольких слоев, расположенных друг за~другом:

\begin{itemize}
  \item сверточный слой с~ядром~3x3
  \item mvn слой
  \item сверточный слой с~ядром~3x3
  \item mvn слой
  \item \dots
  \item maxpool слой с~ядром~2x2 и~шагом~2
\end{itemize}

Всего в~нейронной сети 3~таких блока и~1~блок без~maxpool слоя. Они идут подряд, и~каждый следующий на~выходе дает в~2~раза больше фильтров. 

Структура обратного блока, напротив, нелинейная:

\begin{enumerate}
  \item обратный сверточный слой с~ядром~3x3 и~шагом~2,
  \item сверточный слой с~ядром~1х1 и~шагом~1, который вычисляется от~предыдущего mvn~слоя,
  \item слой, который вычисляет среднее арифметическое из~первых двух слоев блока.
\end{enumerate}

Всего в нейронной сети 3~обратных блока. Количество фильтров в каждой свертке обратного блока равно количеству классов. Если же класса всего~2, то мы задаем на выходе 1~фильтр и~в~конце используем функцию активации sigmoid, вычисляя ошибку при этом с~помощью индекса Дайса. В данной работе во~всех случаях используется только 2~класса.

Обозначим сверточный блок с~$k$~свертками и~$n$~фильтрами как~$conv(k,n)$. Обозначим обратный сверточный слой, который получает на одном из входов $n$~фильтров как~$deconv(n)$. Обозначим слой dropout регуляризации с параметром $p$ как $dropout(p)$. Таким образом, вся архитектура сети выглядит следующим образом:

\begin{itemize}
  \item Вход
  \item $conv(3,64)$
  \item $conv(4,128)$
  \item $conv(4,256)$
  \item $droupout(0.5)$
  \item $conv(4,512)$ без maxpool слоя
  \item $droupout(0.5)$
  \item $deconv(512)$
  \item $deconv(256)$
  \item $deconv(128)$
  \item $sigmoid$ активация
\end{itemize}

Такая архитектура сети была выбрана неслучайно. Сверточные слои обеспечивают вычисление высокоуровневых признаков с помощью применения фильтров к входному изображению. Maxpool слои позволяют уменьшить сложность вычислений, и оставить самую важную информацию, полученную из сверточных слоев. Чтобы сеть сходилась устойчиво, после каждой свертки используется mvn слой~(\cite{batch_norm}). Слой dropout помогает избежать переобучения~(\cite{dropout}). Обратные сверточные слои позволяют получить карту вероятностей принадлежности входных пикселей объекту в~зависимости от~контекста из~высокоуровневых признаков, вычисленных сверточными слоями. Каждый блок обратной свертки связан не только с предыдущим блоком обратной свертки, но и с соответствующим по размеру сверточным блоком, для того, чтобы обеспечить сходимость сети. Несколько обратных слоев позволяют сгладить резкие переходы на~выходной карте вероятностей и~получить более точные предсказания. В~обратных блоках учитываются выходы соответствующих mvn слоев прямых блоков, чтобы увеличить скорость обучения. Этот прием очень похож на~residual блоки в~\cite{resnet}.

\subsection{U-Net}

Архитектура U-Net более сложная. 

\subsection{GridNet}
